[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "New site\n\n\nHello Quarto & Positron\n\n\n\n\n\n\n\n\nAug 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSpencer SCD @ Vandy Part 6: Thoughts about standards\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSpencer SCD at Vandy pt 5: Do we need a central repository for SCD data?\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSpencer SCD @ Vandy pt. 4: What is stability? And consequences of controlling for baseline trend\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSpencer SCD @ Vandy pt. 3: Schools of analysis for SCDs and relevant design choices\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSpencer SCD @ Vandy Part 2: Functional Relation\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nReflections from the Spencer SCD Conference at Vanderbilt Part 1\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-07-02-spencer-scd-vandy-part-6-thoughts-about-standards/index.en.html",
    "href": "posts/2022-07-02-spencer-scd-vandy-part-6-thoughts-about-standards/index.en.html",
    "title": "Spencer SCD @ Vandy Part 6: Thoughts about standards",
    "section": "",
    "text": "BIG DISCLAIMER UP FRONT: These are just my opinions. They are not a reflection of anyone at IES or my colleagues doing work for SWAT contract.\nFor anyone more than passingly familiar with the WWC’s Standards for reviewing SCD studies, they will likely know that the version 4.1 Standards were unpopular with a notable portion of the applied research community. As a part of the move to a fixed-effects meta-analysis framework for synthesizing SCDs, the “evidence standards” part of the Standards was removed in favor of characterizing evidence in terms of the between-case standardized mean difference, called the design-comparable effect size by the WWC. This meant that the Standards no longer included visual analysis. Given the centrality of visual analysis to how nearly all applied SCD researchers understand and analyze their studies, this was a big problem for a lot of them.\nIn my work on the WWC, I have had a lot of opportunity to discuss the Standards with researchers using SCDs, and I have some thoughts about those interactions.\nFirst, my impression is that a non-trivial number of applied researchers don’t have a strong understanding of degree to which the “evidence review” part of the Handbook was not terribly consequential to whether an individual study was described as part of the “evidence base” by the WWC. If you look at the reviews of individual studies page and filter by SCDs and studies which met standards at any level, you won’t see the evidence rating listed anywhere. SCD studies were frequently not part of the primary findings of intervention reports because of the way the WWC interpreted the 5-3-20 rule. It has been implied in at least conversation I’ve witness that the WWC was using the 5-3-20 rule wrong. I don’t know how it was intended to be used, but the practical consequence of how it has been interpreted means that very few SCDs contribute to WWC intervention reports. In any case, there were plenty of intervention reports where there were no primary findings because although some SCDs met Standards, there weren’t enough within a given domain for the WWC to report on them, for instance the System of Least Prompts intervention report.\nEven when the SCD studies met the threshold for the WWC’s interpretation of the 5-3-20 rule, the evidence rating didn’t contribute in the way I think that SCD researchers would have liked it to. Take the primary summary table from the Self-Regulated Strategy Development intervention report:\n\n\n\nprimary summary table self regulated strategy development\n\n\nThe “evidence review” stage ends up being fundamentally a vote-counting exercise in the final outcome analysis. Throughout the intervention report the document says to refer to appendix C for the visual analysis. If you’re an applied SCD researcher, you might expect some narrative discussion of the visual analysis. No. It’s just a summary of the evidence level (i.e., strong, moderate, no evidence), the total attempts to demonstrate a basic effect, and the number of successful attempts.\nI would be willing to be that most applied SCD researchers would not be happy if a meta-analysis published in one of their journals operated this way. And yet…I’ve never personally heard any discussion of this problem. So, it seems likely to me that the objections to the WWC Standards don’t have much to do with the WWC as such.\nSo, what’s the fuss? Based on a lot of conversations I listened to at the conference, a lot of it is about publishing primary research. The published standards exert some pressure on published research, both group designs and SCDs in education. To some degree, this is by design. The WWC would like to help improve the quality of published research. However, there’s a trade-off here. The research and publication cycle is long, so stuff that you see published today may have started work three, four, five years ago. Maybe longer. A lot of this is research that IES itself funded. The WWC doesn’t want to punish researchers for doing something that was best practice when they started their work! This means there is some tension between the evocative effect of spurring better-quality research and the practical desire to include as much reasonable-quality research in the evidence base as possible. I also think there’s reason to be skeptical of the degree to which the WWC can drive primary research practice, except insofar as they influence IES-funded research.\nBut I digress. Prior to my time at the Spencer conference, my suspicion was that another reason for the focus on the WWC’s Standards was because there was a desire to exert control over what is seen as “real” or “good” visual analysis. This is a tricky problem, because the WWC’s Standards (across all designs) are mostly about study design and sometimes study conduct, and not about the study analysis. In fact, a group design study that was well-designed and executed but had garbage analysis could in theory find itself in the evidence base if it reported the right information. The tricky thing with SCDs is that the typical research process has a less clear distinction between the design, conduct, and analysis of studies. Many studies are “designed” as they go, in some sense, because the length of the phases and sometimes other elements of the design are contingent on the pattern of the data observed by the researcher. The applied methods work has not made a clear distinction between those elements of a study that are important for internal validity and those elements that are important for other reasons. I also think they have not made a clear distinction about internal validity for what kind of inference, but this is maybe a separate issue. There is also the problem that much of what applied researchers think of as internal validity checks are intimately connected to the expected or observed value of the intervention effect. Practically speaking, it’s easier for a study with a large effect to be considered “good” via visual analysis. If you’re a typical meta-analyst, you’re probably squirming in your seat right now. If you discard studies with small, null, and negative/contratherapeutic effects, you have the potential for serious publication bias issues.\nSo, to summarize, the WWC doesn’t want to try and dictate analysis and doesn’t really want to make analysis part of their screening. At the end of the day, the WWC’s Standards are Standards for inclusion into synthesis. There are things that aren’t a part of the standards for group designs that are important to writing a high-quality group design paper. I’m sympathetic to the idea that senior researchers want to encourage visual analysis best practices. I had assumed the objections to the remove of visual analysis was a problem of not encouraging all of the right things. What had never been made clear to me before, but what became clear to me at the Spencer conference, was that a big part of the issue researchers took with the WWC Standards was that editors and reviewers treat the Standards prescriptively. If a study doesn’t meet the WWC Standards, it’s considered not worth publishing. The WWC Standards are in this way a straitjacket, rather than gentle evocative guidance for research practices. The natural extension of the worry about the WWC Standards as prescriptive standards is that editors and reviewers, seeing that it’s not a required part of the WWC review process, will no longer give careful consideration of the quality of visual analysis in their review process. A worse outcome from the perspective of many researchers that was never articulated but seems like the logical consequence of following this chain of reasoning is that editors or reviewers may decide that visual analysis should not be a part of the primary studies. Beyond the problem of editors and reviewers, graduate students designing their own studies may lean too heavily on the WWC’s Standards for what constitutes doing a study correctly, rather than thinking carefully about their research questions and the design that will support it.\nAt this point, it’s clear (to me) that there need to be better guidelines on the conduct of primary SCD studies and what editors and reviewers should focus on. Not every publishable SCD study needs to be ready for inclusion into synthesis. Evidence for a particular intervention as defined by the WWC is not the only kind of evidence of value. Plenty of development work that’s part of Goal 2 IES grants is probably worth sharing and publishing, but the qualitative or exploratory work might not qualify as “evidence” in the particular way the WWC focuses on (credit to Ann Kaiser here for pointing some of this out). Standards for primary SCD research will need to reflect this. Standards for primary research should also make clear the continuum of SCD research. Both the research that is “good enough” to be published, even if it isn’t the most rigorously designed and informative study, as well as the “very best” quality research that provides maximum information for the kind of question it’s trying to answer. (Note: This is not my point. Many people at the Spencer conference who were not me said this).\nAs Wendy Machalicek and others noted, there are many “competing” Standards for SCD research that have been published by a variety of research traditions. Traditions here reflect clusters of close collaborators and researchers who share an academic lineage (“academic parent” and “academic child” language weirds me out, but we’ll stick with it for the time being). Researchers common to a particular tradition tend to have a very clear shared understanding of what an SCD study should contain but may not overlap as cleanly with the conventions of another research tradition. However, it is quite likely that there are many more points of agreement than there are points of disagreement. Many points of disagreement may turn out to be semantic rather than substantive. And where there are substantive points of disagreement, it’s likely time there was broader cooperation across the various traditions represented in primary SCD research to articulate what those points of disagreement are and why they matter, alongside a clearer understanding of the shared research values across the traditions."
  },
  {
    "objectID": "posts/2022-05-30-spencer-scd-vandy-pt-4-baseline-trend-stability/index.en.html",
    "href": "posts/2022-05-30-spencer-scd-vandy-pt-4-baseline-trend-stability/index.en.html",
    "title": "Spencer SCD @ Vandy pt. 4: What is stability? And consequences of controlling for baseline trend",
    "section": "",
    "text": "I still don’t know what stability is or how important it is\nJennifer Ledford opened the conference with some important details about the structure and origins of the conference. We were arranged across the rotunda in small tables that seated 8-10, and Jen asked us to talk to our groups about what we thought the big questions or problems were that remained in SCDs. John Ferron and I were sitting at the same table, in no small part because we mostly only knew each other and no one else, and we both immediately tried to ask the same question. What exactly does it mean for a baseline to be stable in SCDs? When SCD authors talk about what drove the choice to intervene at a particular time (mostly they don’t say) it’s common for them to say “we started the intervention when baseline data were stable” or something to that effect. Almost never do they describe what features of the data drove that decision.\nTexts like Jen’s have sections that talk a little bit about the decision to intervene and possible decision rules, but I’m skeptical that the real decision rules are captured there. Not the least of which because I don’t think visual analysis is any one thing but represents a set of related practices that mostly (but not completely) agree. More than that, when I was discussing my problem with Tim Slocum and Wendy I articulated something that I hadn’t been able to articulate before. I suspect that many visual analysts make this choice as a function of practice. They’ve built a decision-making engine in their head that makes choices that are probably reasonably reliable within researchers and across researchers from similar traditions. But they aren’t really making a decision based on a formal decision rule. I think Tim called this the difference between “contingency-based” and “rules-based” behaviors. When you’re asked to describe why you did a contingent behavior as if it were a rule-based behavior, you’re liable to make something up that may or may not relate to the real decision. Related to what I’ve already talked about above, I think that stability can be useful, or at any rate not harmful, when researchers are engaged in inductive research practices. I’m already skeptical that we should be estimating effect sizes from an inductively oriented study, and so the consequences for statistical models applied to response-guided data are maybe irrelevant if you’re estimating effect sizes for the “correct” studies. When you want to estimate an effect size or perform some kinds of hypothesis tests, response-guided data can maybe get you in trouble. Without a really good understanding of what constitutes stability, it’s hard to know what those consequences are, other than that the vague threat looms.\n\n\nTherapeutic trends in baseline: an issue?\nOne thing that seems common in response-guided designs is avoiding a therapeutic baseline phase (this is frequently some element of stability). I have thought about this a lot in the context of effect size estimation and my work on the potential impacts from response-guided designs in SCDs.\nHowever, there are several reasons for an interest in therapeutic trends in baseline that aren’t necessarily related to analysis but have more to do with the choice to intervene at all. I have heard applied researchers say that they don’t want to intervene when kids don’t need it. If the baseline is therapeutic enough the researcher might not just wait until the baseline shows evidence of a pattern of problematic behaviors, they might decline to intervene at all.\nThere are sometimes ethical reasons for this. The intervention might be aversive or even harmful for a student who doesn’t need it. It also might be costly in time and effort for the interventionist, the student, and maybe the family if the student doesn’t need it. However, one thing I haven’t seen discussed as much is that the baseline is an artifact of research! The kids in these studies are not picked randomly, they are selected because someone in their lives (probably parents and/or teachers) think that something is bad enough that help is needed. Absent the need to establish a baseline level of functioning to demonstrate a functional relation, an interventionist (or a teacher or parent who is using an intervention supported by SCD research) is not going to wait to intervene!\nThis means that some proportion of children who would receive the intervention in real life are missing from the SCD literature. If you’re an interventionist, I think you should be asking yourself “What happens to students who don’t need this intervention but receive it anyway?” If the outcomes would be negative, there are contexts where this won’t matter. Most thoughtful applied interventionists will probably cease an intervention that appears ineffective or even harmful relatively quickly. But maybe not all interventionists, especially in contexts where “it gets worse before it gets better” as I heard mentioned a couple of times at the conference. “Naïve” interventionists (teachers, parents etc.) in particular may persist in the face of an iatrogenic response. This seems like a problem! Understanding the responding of individuals who don’t need an intervention but might receive it in practice seems like something that we want in the intervention literature. Although group design work doesn’t necessarily reflect individual negative responses very well, it does contain any aggregated iatrogenic effects in the effect size."
  },
  {
    "objectID": "posts/2022-05-22-spencer-scd-vandy-part-2-functional-relation/index.en.html",
    "href": "posts/2022-05-22-spencer-scd-vandy-part-2-functional-relation/index.en.html",
    "title": "Spencer SCD @ Vandy Part 2: Functional Relation",
    "section": "",
    "text": "I have misunderstood the “functional relation” and potential implications for meta-analysis\nPrior to my time at the conference, I had understood the functional relation as being case-specific, related to the basic effect demonstrated by a case or phase pair. Before, I thought you had a functional relation for an individual whenever you had an observable basic effect that was “big enough” to be considered meaningful. Instead, it quickly became clear to me during the conference that the functional relation is related to both the “big enough” basic effect for each case or phase pair and the demonstration of the three replications in the study or design. Wendy also suggested that the functional relation might not just be a function of, say, an individual treatment/reversal design. If the study has three or six treatment/reversal designs, then the functional relation might be considered to be demonstrated not just by the individual designs but also the replications across the designs in the study.\nThis is a little embarrassing because Wendy Machalicek gently pointed out in a lunch conversation that the present broad understanding of “functional relation” is discussed in the pilot SCD standards which contained visual analysis. I’ll admit to generally being a skim-reader in that I tend to skim text until I hit the part that’s relevant to the specific question I’m interested in. The last time I read the pilot standards beginning to end in detail was a several years ago, and I was worried about things other than what exactly was meant by “functional relation” at the time. I have looked at parts of them a bunch of times in the past few years in my work with the WWC, but never to clarify my understanding of the functional relation because I thought I already understood it.\nI think this understanding of a functional relationship has some implications for meta-analysis, or at least I have some questions. Does a meta-analysis itself potentially demonstrate a functional relation? It’s a collection of designs related by populations, outcomes, and/or interventions. This reminds me of a conversation that James P. and I once had where he suggested that perhaps the WWC should consider meta-analyses of SCDs a “study” for the purposes of the review. I thought it was kind of odd at the time, but maybe not so much now.\nMoreover, can you in some sense rescue designs which are otherwise not useful for causal inference (say, an AB design, or a multiple baseline with only two participants) using meta-analysis? Something like an AB design is still a causal design, it is simply hopelessly subject to unobserved confounds by itself. If you’ve got enough of them, and you perform meta-analysis, do you have a reasonable estimate of the causal effect? Thanks to Joe Lambert for making me think about this. As an aside, the vagueness over what exactly constitutes a functional relation worries me a little bit. I’m not sure if I’m worried for good reason, because I can’t really describe anything other than formless concern. Maybe just because wherever things are vague in research design or analysis, I start to worry about unforeseen consequences."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel M. Swan",
    "section": "",
    "text": "I am research methodologist and data scientist. I like to help people people produce their best research. I have been programming and working with data in R for over ten years. I also have extensive experience with research synthesis projects, particularly evidence clearinghouse projects for the federal government. I am a named contributor to two versions of the What Works Clearinghouse handbooks, worked on the official training system, and also trained reviewers to read and code studies for systematic review and meta-analysis projects. As a graduate student, I was trained in research methods, statistics, and causal inference."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Daniel M. Swan",
    "section": "Education",
    "text": "Education\nUniversity of Texas at Austin | Austin, TX Phd and MEd in Educational Psychology | 2019\nUniversity of Texas at Dallas | Richardson, TX B.A. in Psychology | 2010"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Daniel M. Swan",
    "section": "Experience",
    "text": "Experience\nAbt Global | Senior Analyst | October 2023 - March 2025\nUniverity of Oregon | Research Associate | October 2018 - May 2023"
  },
  {
    "objectID": "posts/2022-05-21-reflections-from-the-spencer-scd-conference-at-vanderbilt-part-1/index.en.html",
    "href": "posts/2022-05-21-reflections-from-the-spencer-scd-conference-at-vanderbilt-part-1/index.en.html",
    "title": "Reflections from the Spencer SCD Conference at Vanderbilt Part 1",
    "section": "",
    "text": "This was important enough to me that it seemed worth posting my first real blog about. This first part of my reflections are more about how the experience at Vanderbilt made me feel and the general impressions I got from being there than it is direct thoughts or questions about single-case designs. Apologies if this is not very well-organized, I’m adapting some stream of consciousness notes.\nThis past Wednesday through Friday I attended a small single-case design conference organized by Jennifer Ledford and held at Vanderbilt, funded by the Spencer foundation. I’ll say up front that I hope this is the first of these conferences rather than the only conference. I attended for a lot of reasons, but the biggest reason was that while my work and research has focused heavily on SCDs, I have very little opportunity to interact with researchers who use SCDs. As a methodologist, it’s really easy to become wrapped up in problems that don’t really relate to the things that applied researchers are thinking about. It’s also really easy to miss important areas of research that you wouldn’t otherwise know exist.\nThe conference was small, but I got to meet a lot of people whose names I know and whose work is very important to my understanding of SCDs. I also learned, to my surprise, that people knew who I was. Perhaps I am not an idiot. I got to talk with a lot of people who are thinking very critically about the work they do. I believe subtle misunderstandings that I had about the design and about the people doing this work were corrected by spending several days with researchers and faculty. I’ll talk about one of these in a later post, but I’m sure there are changes in my thinking I’m not even aware of.\nAfter the conference ended at noon on Friday, I begged off any more face time with all the new colleagues I met and went and sat in the shade some trees on the Vanderbilt campus and just listened to the sound of the leaves and the wind and tried to let my brain settle. Then I went to Chuy’s (a comforting reminder of time in Austin) for a quiet lunch by myself. Afterwards, I went to the lobby of the hotel where I had been staying and spent almost two hours writing down my thoughts. Then I went to the airport, and spent another few hours writing more thoughts. Saturday morning I got up and puttered around, and then spent another hour or so writing more thoughts. In all, I’ve got about nine pages of stream of consciousness that feels like it’s best as a series of blog posts. Anyway, here are some vague impressions I got:\n\nApplied SCD researchers think very hard about measurement\nI don’t know that this should be surprising, it just had never occurred to me. It’s just that so many grad students (and to a lesser extent early career researchers) who I worked with in group designs didn’t really worry so much about measurement. They tend to select a measure that exists and is commonly used, without a lot of consideration for how appropriate it is for their purposes. If you select a measure intended to capture a relatively steady feature of personality or behavior, it’s going to be hard to move that behavior. (Ann Kaiser and Wendy Machalicek both independently made similar points about this).\nSometimes when some group design researchers do think about measurement, maybe they do in the wrong way, trying to make up their own measure without worrying about validity issues because they don’t think anything existing is suitable. In any case, it’s clear that they think & worry a lot about measurement and the shortcoming in their measures. I heard a lot of offhand discussion about the problem of the quality of social validity measures.\n\n\nGrad students (and faculty) are very engaged with complicated methodological work that they may view as difficult but take seriously.\n\n(Also some of the grad students know my name?? aka possibly I am not an idiot[!!])\nOk, so, when I first emailed Jen about attending the conference her response to me she was immediately welcoming and she wrote that she had “read [my] work.” Which was nice, but I kind of assumed wasn’t about me per say. I’ve got a couple of first author publications, but the papers I’m on that I think get cited most are all the papers where James Pustejovsky is first author. This is understandable, James is casually brilliant whereas I am merely sometimes clever. I’m comfortable with that.\nHowever, during the first round table session on the very first day there were a few grad students sitting near me and I was chatting with them. One of them looked at my name tag and asked excitedly “Wait, are you the Swan from statistics??” That made my brain glitch out for a second because I’m not really a statistician. Statisticians are mathematicians with a perverse interest in empirical things. James is a statistician. I’m a methodologist, which means I only (badly) do calculus and linear algebra when absolutely required rather than recreationally.\nSo, look, pendatic aside aside, yes I was the Swan from statistics. It turns out Jen teaches an Advanced SCD course where they read my first author work. This was the first time I’d ever met anyone who had read my work outside methodologists. Corey Peltier told me once by e-mail, but it’s another thing entirely for a grad student to come over very politely and ask “Dr. Swan” if they can ask a question about a project they are working on. It was a meta-analysis question, so to hide my incredulity I made Man Chen come over and help me answer it because she truly knows way more about meta-analysis than I do, despite not being “Dr. Chen” yet.\nMore broadly, though, it was clear to me that despite many of these researchers viewing the work my colleagues and I do with some trepidation, they were working hard to take advantage of the advances that methodologists have offered them over the years. Not only did they want to use all the tools they had, there was an active interest in the tools they didn’t yet have (for instance, a between-case standardized mean difference for alternating treatment designs) in a way that I hadn’t really anticipated. Methodologists interested in SCDs who aren’t looking at applied researchers as eager partners in their work are probably missing out badly.\n\n\n\nBehaviorism is alive and well\nMy undergraduate background in psychology kind of treated behaviorism as a dead tradition. I knew in an abstract way that SCDs arose out of the behaviorist tradition and that ABA and SCD had a close link. But I wasn’t really ready for how committed a lot of folks were. Certainly, none of the folks I met here were (or at least professed to be) hardcore Skinner-type behaviorists. But most of them were ABA certified or on their way there, and that behaviorist orientation is a profound part of their understanding of how to help children, students, and sometimes adults.\nThere was also a keen understanding among some of the people I talked to of the epistemological grounding of their research tradition, particularly compared to what I see from many group design researchers. I worry about the epistemology of quantitative research these days, so it was interesting to hear. I suspect it might be consequential in ways that aren’t clear to me yet, so I’ll have to keep it in mind.\nFuture posts in this series will cover more substantive topics:\n\nSchools of Analysis and Research Design Choices (Thanks John Ferron)\nPossible consequences of attention to therapeutic trends in baseline\nI have misunderstood the “functional relation” and its implications for meta-analysis\nI still don’t know what stability is or how important it is\nDo we need a central repository for SCD data?\n\nPossibly some of these topics will be covered together. I don’t know exactly yet."
  },
  {
    "objectID": "posts/2022-05-23-scd-spencer-vandy-pt-3-schools-of-analysis-for-scds/index.en.html",
    "href": "posts/2022-05-23-scd-spencer-vandy-pt-3-schools-of-analysis-for-scds/index.en.html",
    "title": "Spencer SCD @ Vandy pt. 3: Schools of analysis for SCDs and relevant design choices",
    "section": "",
    "text": "Schools of Analysis and Research Design Choices\nWhen I was developing my own review of literature for the analysis of SCDs for my qualifying process and later dissertation, I sorted the analysis of SCDs into roughly four buckets.\n\nVisual analysis\nNon-overlap indices\nModel-based approaches (including simple models for effect sizes)\nRandomization tests\n\nJohn Ferron gave a sort of opening talk about the analysis of SCDs. The talk was helpful to situate many of the discussions throughout the day. He proposed three broad kinds of analysis in SCDs that I found interesting, in part because it was similar to but also different from the way that I have mentally divided up analysis of SCDs in the past. He proposed three general classes of analysis:\n\nVisual analysis\nEffect size estimation\nNull hypothesis testing\n\nI think John’s organization is better for a way of looking at SCDs that I’ve been contemplating than my own scheme above. These three forms of are appropriate for related, but not entirely equivalent questions.\nVisual analysis reflects a desire to know whether some outcome (usually behavior) has changed enough in a way that both matters and replicates. I have some thoughts about what constitutes replication (I think I mentioned this in the last post?) that I should write about later but suffice it to say that I think the idea of replication in SCDs is tricky and interesting in a way that is kind of specific to the visual analysis tradition but might also relate to issues in many other research design and analysis contexts.\nEffect size estimation asks, “how big is the effect of the intervention?” Coupled with meta-analysis, we can ask further questions about treatment variability, heterogeneity, and structural factors that impact the how big the intervention effect is. John grouped non-overlap indices in this bucket, but I think I would put them closer to the visual analysis bucket. Non-overlap indices are insensitive to magnitude. Once there’s no overlap between phases, the difference between a small effect and a large effect cannot be observed based on nonoverlap effects alone. This is somewhat parallel to the idea that I think I see in visual analysis (although I am by no means an expert) that cares less about the precise magnitude of the effects than whether it is big enough.\nNull hypothesis testing asks the question “are the observed effects unlikely to have occurred if the true effect is zero?” or more colloquially (if maybe not quite correctly) “are the effects different from zero?” Null hypothesis testing is perhaps less concerned with the exact value of the outcome and more concerned that an effect is “real” in some probability-related sense. The effect size estimation and null hypothesis testing approaches are often used in tandem, although not always. Many of the randomization approaches that have been proposed for SCDs do not directly include a component of effect size estimation.\nAs we consider these different types of analysis, I think it’s helpful to bring up the inductive/deductive continuum offered by Brian Cook and Austin Johnson. To paraphrase the description, rather than a single unified approach, SCDs encompass a set of research practices that answer a variety of questions. Some SCDs are highly inductive, engaging in an open-ended and flexible investigation of how the researcher or clinician might be able to move the behavior of someone experiencing distress or exhibiting behaviors that are causing some sort of social difficult. In contrast, some SCDs are highly deductive, trying to better understand the consequences of some well-defined intervention. Many (perhaps most?) SCDs fall somewhere in the middle of this continuum of research questions. But if you look at the variety of analysis approaches and this continuum of investigation style, you may begin to see (or at least, I have recently begun believe) that SCDs are asking a variety of questions, and the kind of inferences they are making are not all the same. Consequently, it may be that an effect size from a highly inductive study may not be all that interpretable, because the data from the intervention phase are not really all from the same “intervention.” It also seems to me that research practices that are helpful for certain kinds of inductive studies are not entirely helpful for deductive studies. Research practices which may support clarity in visual analysis may represent a problem for effect size estimation or null hypothesis tests (see, for instance, my work on simulated response-guided baselines).\nSome practices are taken as a given, I think. For researchers who use response-guided baselines, the question of whether or not to be response-guided isn’t a question: of course you are always gathering data in a response-guided fashion! But I’m not sure this is the right response. This is a research design question that you should consider carefully. It might be that there are ethical reasons that require you be responsive, but you should recognize that as a design choice it isn’t always optimal. Given the other talks there that thoughtfully challenged conventional wisdom about what is or is not the best design choice in SCDs (such as Tim Slocum and Jennifer Ledford’s dual talk on nonconcurrent multiple baseline designs) I hope we are entering a time where we can be more thoughtful about the kind of inference researchers are interested in making in their SCD, the kind of analysis that can answer their question, and the sorts of research practices that can support (or harm!) understanding the evidence from their research."
  },
  {
    "objectID": "posts/2022-07-01-do-we-need-a-central-repository-for-scd-data/index.en.html",
    "href": "posts/2022-07-01-do-we-need-a-central-repository-for-scd-data/index.en.html",
    "title": "Spencer SCD at Vandy pt 5: Do we need a central repository for SCD data?",
    "section": "",
    "text": "Open research practices are of keen interest to a lot of the folks I met, particularly the grad students. I think that some kinds of open research practices are a little more natural to SCD researchers, probably because they already share their raw data as a matter of course in the form of plots for visual analysis. Brian Cook talked about open data during his open research talk, and data sharing came up again when James talked about meta-analysis. I wonder about a central location for data sharing in SCDs. In some work of James’ that I collaborated on, we collected data from seven synthetic reviews to do our own analysis, and it’s available on OSF. In the same vein, two different researchers at the conference told me that had a bunch of data they wanted to make freely available when they were done extracting it from plots, because there was a lot of information available to answer questions they were never going to ask. Clearly, there’s an interest in sharing data. OSF is maybe a natural answer to this, but you have to know a dataset exists before you can look for it. A central location for primary and secondary data sources in SCDs seems like it would be of special interest.\nBeyond that, there’s maybe an interest in talking about how to organize and structure this data. Data from studies always has some nesting because there are features common to groups, and then usually features common to condition when authors report things like balance tests. Things get trickier in SCDs, because you have observations nested in cases, and sometimes cases nested in designs (with multiple baseline and multiple probe designs) as well as these designs nested in studies, and in this dataset studies were nested in review. James arrived at a sort of organizing principle of relational databases for the data that worked alright, with the biggest failure being the choice to try and put it into Microsoft Access (never do this!!). This is not a post about Access, but I bet some web searches would yield plenty of information about the many failings of Access. I think the idea of a relational database is cool, but in practice I have a copy of the data where I duplicated all the review, study, design, and case level data down to the observation level, because it’s easy enough to get summaries for higher-level units in R using the tidyverse.\nFinally, I think there are some interesting opportunities available. I’m sure we’ve got some replicated extraction going on out there. Previous work has studied the reliability of the extraction process, but it would be very cool to run reliabilities across data from independent research groups. Such a repository can provide guidance on how to organize datasets for sharing. Jason Chow noted the possibility of providing some guidance on costs associated with archiving data, so folks seeking grants can write it into their grant requests. That hadn’t occurred to me, but is a really cool idea.\nJason also mentioned LDbase (https://ldbase.org/). It’ a “behavioral data repository” with a project-focused setup for sharing data, code, and documentation in a hierarchical structure. I poked around a little bit and didn’t feel like I had a really good handle on what was available there, but maybe this is the right place to share data? When I chatted with my Emily about my experiences, she encouraged me not to duplicate effort. I certainly don’t want to try and re-do someone else’s hard work. I’d be interested in hearing other people’s thoughts."
  },
  {
    "objectID": "posts/2025-08-19-new-website/index.html",
    "href": "posts/2025-08-19-new-website/index.html",
    "title": "New site",
    "section": "",
    "text": "Like many other people in the R Extended Universe circa 2021, I originally created my website using blogdown and Hugo. In a lot of ways, it was great! It worked with minimal fiddling, integrated R code and markdown, etc. However, the way that Hugo worked meant that if you didn’t keep up with your site, it was pretty easy to get stuck with a site that was hard to update. This is one of the reasons that I haven’t really touched my site since 2022. Updating it became kind of a pain. Enter Quarto, which is a sort of replacement/extension of the R Markdown document supported by Posit. There’s still some fiddling, but overall it wasn’t too painful to port the important stuff from my old website to this format. For reference, I used the following blog posts to help bootstrap my process:\n\nhttps://www.andreashandel.com/posts/2022-10-01-hugo-to-quarto-migration/\nhttps://blog.djnavarro.net/posts/2022-04-20_porting-to-quarto/#fnref1\n\nI also poked around the github for my Ph.D. advisor’s site (https://github.com/jepusto/jepusto-quarto) to see how he’d organized things.\nIn addition to shifting to Quarto, I’m using Positron instead of the familiar RStudio. If you’re stuck in the past (like I was, just a few weeks ago) Positron is a new data science IDE that supports both R and Python natively. It’s based on VSCode, but looks like RStudio. This is comforting if you’ve been using RStudio for about ten years. I’ve tried to stick my feet into the VSCode pool, but it can be disorienting if I’m also doing something else that I’m not as familiar with, like code in Python. One of the things I need to brush up on is the Python toolchain, like NumPy, Pandas, Scikit-Learn. Maybe Seaborn. I feel like I could probably still use ggplot2 for visualization, even I need to use Python for the rest of the toolchain. But I digress. Upskilling Python will be a lot easier with an IDE that feels comfortable.\nAt the time of this post, things are still under construction. For instance, I don’t yet have a section for my publications or my resume. However, I wanted to get a new version this up and ready to go ASAP. More to come soon."
  }
]